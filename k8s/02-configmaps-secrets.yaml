---
# ConfigMap for Fluent Bit configuration (aligned with docker-compose)
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: waf-system
data:
  fluent-bit.conf: |
    # Simplified Fluent Bit collecting ModSecurity JSON and sending to Kafka
    [SERVICE]
        Flush         5
        Daemon        off
        Log_Level     info
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    [INPUT]
        Name              tail
        Path              /var/log/modsecurity/*.json
        Tag               waf.modsec
        Parser            json
        Refresh_Interval  1
        Read_From_Head    false
        Skip_Long_Lines   On

    [FILTER]
        Name              modify
        Match             waf.modsec
        Add               processed_timestamp ${HOSTNAME}-${TIMESTAMP}

    [FILTER]
        Name              modify
        Match             waf.modsec
        Add               track analytics
        Add               rule_id ""

    [OUTPUT]
        Name        kafka
        Match       waf.modsec
        Brokers     kafka.waf-processing.svc.cluster.local:9092
        Topics      waf-realtime-events
        Format      json
        Retry_Limit 3

  parsers.conf: |
    [PARSER]
        Name        json
        Format      json
        Time_Keep   On

---
# Secret for authentication - TEMPLATE VERSION
apiVersion: v1
kind: Secret
metadata:
  name: waf-auth-secrets
  namespace: waf-system
type: Opaque
data:
  # values - replace these with your actual credentials
  google-client-id: ${GOOGLE_CLIENT_ID}  # your Google Client ID
  google-client-secret: ${GOOGLE_CLIENT_SECRET}  # your Google Client Secret
  jwt-secret: ${JWT_SECRET}  # your JWT secret (32 bytes recommended)
  nextauth-secret: ${NEXTAUTH_SECRET}  # your NextAuth secret

---
# ConfigMap for InfluxDB token and defaults - TEMPLATE VERSION
apiVersion: v1
kind: ConfigMap
metadata:
  name: influxdb-config
  namespace: waf-data
data:
  influxdb-token: "${INFLUXDB_TOKEN}"
  influxdb-org: "${INFLUXDB_ORG}"
  influxdb-bucket: "${INFLUXDB_BUCKET}"

---
# ConfigMap for Kafka topics script (expanded topics to match compose + DDL)
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-topics-script
  namespace: waf-processing
data:
  ensure-topics.sh: |
    #!/bin/bash
    set -e

    echo "Waiting for Kafka to be ready..."
    until kafka-topics --bootstrap-server "$BROKER" --list >/dev/null 2>&1; do
      echo "Kafka not ready yet..."
      sleep 2
    done
    echo "Creating topics..."

    create() {
      local t="$1"; local p="$2"; local rf="$3"; shift 3 || true
      if kafka-topics --bootstrap-server "$BROKER" --create --if-not-exists --topic "$t" --partitions "$p" --replication-factor "$rf" "$@"; then
        echo "Ensured topic $t"
      fi
    }

    # From docker-compose ensure-topics.sh
    create waf-logs              6 1
    create waf-modsec-raw        6 1
    create waf-modsec-enriched   6 1
    create waf-modsec-metrics    3 1
    create waf-realtime-events   3 1 --config retention.ms=604800000
    create waf-rulemap           6 1 --config cleanup.policy=compact
    create waf-archive           3 1 --config retention.ms=2592000000

    # From ksqlDB DDL
    create waf-attack-alerts     3 1
    create waf-block-alerts      3 1
    create waf-uri-alerts        3 1
    create waf-alerts            3 1

    echo "Topics created:"
    kafka-topics --bootstrap-server "$BROKER" --list

---
# ConfigMap for ksqlDB DDL (from repo)
apiVersion: v1
kind: ConfigMap
metadata:
  name: ksqldb-ddl
  namespace: waf-processing
data:
  ddl.sql: |
    -- Simplified DDL for Analytics Track Processing
    -- Maps ModSecurity events to enriched analytics data for Kibana visualization

    -- 1) RAW Stream from Fluent Bit (simplified structure)
    CREATE STREAM MODSEC_RAW (
      transaction STRUCT<
        client_ip STRING,
        time_stamp STRING,
        request STRUCT<
          method STRING,
          uri STRING,
          headers MAP<STRING, STRING>
        >,
        response STRUCT<
          http_code INT
        >,
        messages ARRAY<STRUCT<
          message STRING,
          details STRUCT<
            ruleId STRING,
            match STRING,
            data STRING,
            severity STRING
          >
        >>
      >,
      track STRING,
      processed_timestamp STRING
    ) WITH (
      KAFKA_TOPIC='waf-realtime-events',
      VALUE_FORMAT='JSON'
    );

    -- 2) Analytics Stream (filter analytics track only)
    CREATE STREAM MODSEC_ANALYTICS AS
    SELECT
      transaction->client_ip AS client_ip,
      transaction->time_stamp AS time_stamp,
      transaction->request->method AS method,
      transaction->request->uri AS uri,
      transaction->response->http_code AS status,
      track
    FROM MODSEC_RAW
    WHERE track = 'analytics'
    EMIT CHANGES;

    -- 3) Output to analytics topic for Logstash consumption
    CREATE STREAM MODSEC_FOR_KIBANA
    WITH (
      KAFKA_TOPIC='waf-logs',
      VALUE_FORMAT='JSON'
    ) AS
    SELECT
      client_ip,
      time_stamp AS ts,
      method,
      uri,
      status,
      'analytics' AS classification_track
    FROM MODSEC_ANALYTICS
    EMIT CHANGES;

    -- 4) Real-time anomaly detection and alerting

    -- High frequency attack detection (>50 requests per minute)
    CREATE STREAM HIGH_FREQUENCY_ATTACKS
    WITH (
      KAFKA_TOPIC='waf-attack-alerts',
      VALUE_FORMAT='JSON'
    ) AS
    SELECT
      client_ip,
      COUNT(*) as request_count,
      COLLECT_LIST(uri) as attacked_uris,
      WINDOWSTART as alert_timestamp,
      'HIGH_FREQUENCY_ATTACK' as alert_type,
      'CRITICAL' as severity,
      'Client exceeded 50 requests per minute' as description
    FROM MODSEC_ANALYTICS
    WINDOW TUMBLING (SIZE 1 MINUTE)
    GROUP BY client_ip
    HAVING COUNT(*) > 50
    EMIT CHANGES;

    -- Block rate spike detection (>30% blocked in 5 minutes)
    CREATE STREAM BLOCK_RATE_ALERTS
    WITH (
      KAFKA_TOPIC='waf-block-alerts',
      VALUE_FORMAT='JSON'
    ) AS
    SELECT
      WINDOWSTART as alert_timestamp,
      COUNT(*) as total_requests,
      SUM(CASE WHEN status >= 400 THEN 1 ELSE 0 END) as blocked_requests,
      CAST((SUM(CASE WHEN status >= 400 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS INT) as block_rate_percent,
      'BLOCK_RATE_SPIKE' as alert_type,
      'WARNING' as severity,
      'Block rate exceeded 30% threshold' as description
    FROM MODSEC_ANALYTICS
    WINDOW TUMBLING (SIZE 5 MINUTES)
    GROUP BY WINDOWSTART
    HAVING (SUM(CASE WHEN status >= 400 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) > 30
    EMIT CHANGES;

    -- Error-only stream for URI attack analysis
    CREATE STREAM MODSEC_ERRORS AS
    SELECT *
    FROM MODSEC_ANALYTICS
    WHERE status >= 400
    EMIT CHANGES;

    -- Top attacked URIs (>10 requests per URI in 5 minutes)
    CREATE STREAM TOP_ATTACKED_URIS
    WITH (
      KAFKA_TOPIC='waf-uri-alerts',
      VALUE_FORMAT='JSON'
    ) AS
    SELECT
      uri,
      COUNT(*) as attack_count,
      COUNT_DISTINCT(client_ip) as unique_attackers,
      WINDOWSTART as alert_timestamp,
      'URI_ATTACK_SPIKE' as alert_type,
      'HIGH' as severity,
      'URI under heavy attack from multiple sources' as description
    FROM MODSEC_ERRORS
    WINDOW TUMBLING (SIZE 5 MINUTES)
    GROUP BY uri
    HAVING COUNT(*) > 10
    EMIT CHANGES;

    -- Unified alerts topic (combines all alert types)
    CREATE STREAM UNIFIED_WAF_ALERTS
    WITH (
      KAFKA_TOPIC='waf-alerts',
      VALUE_FORMAT='JSON'
    ) AS
    SELECT
      alert_type,
      severity,
      client_ip,
      CAST(NULL AS STRING) as uri,
      request_count as metric_value,
      alert_timestamp,
      description
    FROM HIGH_FREQUENCY_ATTACKS
    EMIT CHANGES;

    -- Insert block rate alerts into unified stream
    INSERT INTO UNIFIED_WAF_ALERTS
    SELECT
      alert_type,
      severity,
      CAST(NULL AS STRING) as client_ip,
      CAST(NULL AS STRING) as uri,
      block_rate_percent as metric_value,
      alert_timestamp,
      description
    FROM BLOCK_RATE_ALERTS
    EMIT CHANGES;

    -- Insert URI attack alerts into unified stream
    INSERT INTO UNIFIED_WAF_ALERTS
    SELECT
      alert_type,
      severity,
      CAST(NULL AS STRING) as client_ip,
      uri,
      attack_count as metric_value,
      alert_timestamp,
      description
    FROM TOP_ATTACKED_URIS
    EMIT CHANGES;

  init-ksqldb.sh: |
    #!/bin/bash
    # ksqlDB ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ (from repo)
    set -e

    KSQLDB_URL="${KSQLDB_URL:-http://ksqldb.waf-processing.svc.cluster.local:8088}"
    DDL_FILE="/scripts/ddl.sql"
    MAX_RETRIES=30
    RETRY_INTERVAL=10

    echo "ðŸš€ Starting ksqlDB initialization..."

    wait_for_ksqldb() {
        local retries=0
        echo "â³ Waiting for ksqlDB to be ready..."
        while [ $retries -lt $MAX_RETRIES ]; do
            if curl -s -f "$KSQLDB_URL/info" >/dev/null 2>&1; then
                echo "âœ… ksqlDB is ready!"
                return 0
            fi
            retries=$((retries + 1))
            echo "   Attempt $retries/$MAX_RETRIES failed. Retrying in ${RETRY_INTERVAL}s..."
            sleep $RETRY_INTERVAL
        done
        echo "âŒ ksqlDB failed to become ready after $MAX_RETRIES attempts"
        return 1
    }

    execute_ddl() {
        echo "ðŸ“ Executing DDL script..."
        if [ ! -f "$DDL_FILE" ]; then
            echo "âŒ DDL file not found: $DDL_FILE"
            return 1
        fi
        if ksql "$KSQLDB_URL" --file "$DDL_FILE"; then
            echo "âœ… DDL executed successfully"
            return 0
        else
            echo "âŒ DDL execution failed"
            return 1
        fi
    }

    verify_streams() {
        echo "ðŸ” Verifying stream creation..."
        local expected_streams=("MODSEC_RAW" "MODSEC_ANALYTICS" "MODSEC_FOR_KIBANA")
        local streams_output
        streams_output=$(ksql "$KSQLDB_URL" --execute "SHOW STREAMS;" 2>/dev/null || echo "")
        local missing_streams=()
        for stream in "${expected_streams[@]}"; do
            if ! echo "$streams_output" | grep -q "$stream"; then
                missing_streams+=("$stream")
            fi
        done
        if [ ${#missing_streams[@]} -eq 0 ]; then
            echo "âœ… All required streams created successfully"
            for stream in "${expected_streams[@]}"; do
                echo "   - $stream"
            done
            return 0
        else
            echo "âŒ Missing streams: ${missing_streams[*]}"
            return 1
        fi
    }

    verify_topics() {
        echo "ðŸ” Verifying Kafka topics..."
        local expected_topics=("waf-logs" "waf-alerts")
        sleep 5
        for topic in "${expected_topics[@]}"; do
            echo "   Checking topic: $topic"
        done
        echo "âœ… Topic verification completed"
    }

    main() {
        echo "======================================"
        echo "     ksqlDB Initialization Script     "
        echo "======================================"
        echo ""
        if ! wait_for_ksqldb; then exit 1; fi
        echo ""
        if ! execute_ddl; then echo "âŒ DDL execution failed, but continuing..."; fi
        echo ""
        if ! verify_streams; then echo "âš ï¸  Some streams may be missing, but initialization completed"; fi
        echo ""
        verify_topics
        echo ""
        echo "ðŸŽ‰ ksqlDB initialization completed successfully!"
        echo "======================================"
        return 0
    }

    main "$@"

---
# ConfigMap for Logstash pipeline (from repo, adapted for k8s services)
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-pipeline
  namespace: waf-processing
data:
  pipeline.conf: |
    input {
      # Keep Kafka input for analytics pipeline
      kafka {
        bootstrap_servers => "kafka.waf-processing.svc.cluster.local:9092"
        topics            => ["waf-logs"]
        group_id          => "ls-waf-analytics"
        auto_offset_reset => "earliest"
        codec             => "json"
        decorate_events   => true
      }
    }

    filter {
      # Process ksqlDB/Kafka input format
      if [@metadata][kafka][topic] == "waf-logs" {
        mutate {
          add_tag => [ "kafka_input", "analytics_track" ]
          rename => { "CLIENT_IP" => "[client][ip]" }
          rename => { "TS" => "[event][original_timestamp]" }
          rename => { "METHOD" => "[http][request][method]" }
          rename => { "URI" => "[url][path]" }
          rename => { "STATUS" => "[http][response][status_code]" }
          rename => { "CLASSIFICATION_TRACK" => "[labels][track]" }
        }
      }

      mutate {
        remove_field => ["@version", "source", "host"]
      }
    }

    output {
      elasticsearch {
        hosts => ["http://elasticsearch.waf-data.svc.cluster.local:9200"]
        index => "waf-logs-%{+YYYY.MM.dd}"
        ilm_enabled => false
      }
    }